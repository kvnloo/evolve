# AI/ML Pipeline Quick Start Guide

**Transform bleeding-edge AI/ML research into production code in 5.75 hours (vs 20-30 hours manual)**

## ğŸ¯ What This Pipeline Does

Automatically converts:
- YouTube videos about AI/ML
- Research papers (PDFs)
- Blog posts

Into:
- Working, tested code
- Framework integrations (Claude Flow, SuperClaude, CCPM)
- Production-ready PRs

## âš¡ Quick Example

```bash
# Single command - full autonomous pipeline
/automation:ai-pipeline "https://youtube.com/watch?v=..." --depth standard

# Pipeline executes:
# âœ… Phase 1: Concept extraction (3 min)
# â¸ï¸  Phase 2: PRD generation â†’ Human approval (15 min + 10 min review)
# âœ… Phase 3: Task decomposition â†’ GitHub Issues (5 min)
# âœ… Phase 4: Parallel implementation (2-4 hours, 3-8 agents)
# â¸ï¸  Phase 5: Multi-agent code review â†’ PR (45 min + 15 min review)

# Total: 5.75 hours | Human time: 25 minutes (just 2 approvals)
```

## ğŸ“‹ Prerequisites

### Required
- âœ… Claude Flow MCP server installed: `claude mcp add claude-flow npx claude-flow@alpha mcp start`
- âœ… GitHub CLI authenticated: `gh auth login`
- âœ… Git repository initialized with remote

### Optional (Enhanced Features)
- Flow-Nexus for distributed training: `claude mcp add flow-nexus npx flow-nexus@latest mcp start`
- Ruv-Swarm for advanced coordination: `claude mcp add ruv-swarm npx ruv-swarm mcp start`

## ğŸš€ Your First Pipeline Run (Step-by-Step)

### Option 1: Fully Automated (Recommended)

**1. Pick a simple YouTube video** (e.g., "React Server Components explained")
```bash
/automation:ai-pipeline "https://youtube.com/watch?v=PRQCAL_RMVo" --depth standard
```

**2. Wait ~3 minutes** - Concept extraction runs automatically
```
âœ… Extracted concepts: React Server Components, data fetching, streaming
âœ… Frameworks identified: React, Next.js
âœ… Integration points: Could add new SPARC mode or example
```

**3. Review PRD (~5 min)** - Human approval gate
```
ğŸ“„ PRD generated: .claude/prds/react-server-components.md
Review and approve:
  - Technical requirements accurate?
  - Integration approach makes sense?
  - Success criteria clear?

âœ… Approve to continue
```

**4. Automatic decomposition** (~5 min)
```
âœ… Created 8 GitHub Issues
âœ… Dependency graph mapped
âœ… Git worktree created: worktrees/epic-react-server-components
âœ… Agents assigned to streams
```

**5. Watch parallel implementation** (2-4 hours)
```
ğŸ¤– Stream A: Algorithm implementation (2 agents)
ğŸ¤– Stream B: Framework integration (2 agents)
ğŸ¤– Stream C: Testing (2 agents)
ğŸ¤– Stream D: Documentation (1 agent)

Monitor progress:
  /monitoring:status
  /pm:epic-status
```

**6. Review PR (~10 min)** - Final approval gate
```
ğŸ“‹ PR created with:
  âœ… Security review: No vulnerabilities
  âœ… Performance review: Benchmarks look good
  âœ… Architecture review: Clean integration
  âœ… 87% test coverage
  âœ… All quality gates passed

âœ… Merge to deploy
```

### Option 2: Phase-by-Phase Control

**Phase 1: Extract Concept**
```bash
/automation:ai-pipeline:extract "https://youtube.com/watch?v=..."

# Review extracted concept
Read .claude/memory/ai-pipeline/concepts/latest.json
```

**Phase 2: Generate PRD**
```bash
/automation:ai-pipeline:prd "concept-name"

# Review PRD
/pm:prd-show "concept-name"

# If good, proceed. If needs changes, edit PRD then continue
```

**Phase 3: Decompose into Tasks**
```bash
/automation:ai-pipeline:decompose "concept-prd-id"

# Check GitHub Issues
gh issue list --label "ai-ml"

# Check worktree
ls worktrees/
```

**Phase 4: Implement**
```bash
/automation:ai-pipeline:implement "concept-epic-id"

# Monitor progress
/monitoring:agents
/pm:epic-status
```

**Phase 5: Validate & Deploy**
```bash
/automation:ai-pipeline:validate "pr-number"

# Review PR
gh pr view <number>

# Merge
gh pr merge <number>
```

## ğŸ’¡ Best Practices

### Start Simple
1. **First run**: Pick a short (~10 min) YouTube video on a simple concept
2. **Second run**: Try a blog post with code examples
3. **Third run**: Tackle a research paper abstract (not full paper yet)

### Gradual Complexity
```yaml
week_1_2:
  - Simple React pattern â†’ New example
  - Small optimization technique â†’ Helper function
  - Testing strategy â†’ Test template

week_3_4:
  - Complete algorithm â†’ New SPARC mode
  - Agent pattern â†’ New agent type
  - Framework integration â†’ MCP tool wrapper

week_5_8:
  - Full research paper â†’ Complete system
  - Novel architecture â†’ New subsystem
  - Distributed system â†’ Multi-package implementation
```

### Quality Gates

**Always verify at approval gates:**

âœ… **PRD Review Checklist**:
- [ ] Problem statement clear?
- [ ] Solution approach makes sense?
- [ ] Integration points identified correctly?
- [ ] Success criteria are measurable?
- [ ] No security concerns?

âœ… **PR Review Checklist**:
- [ ] All tests passing?
- [ ] Code coverage >85%?
- [ ] Documentation complete?
- [ ] No breaking changes?
- [ ] Performance acceptable?

## ğŸ“ Example Workflows

### Workflow 1: YouTube â†’ New SPARC Mode
```bash
# Source: "Advanced LLM reasoning with Chain of Thought"
/automation:ai-pipeline "https://youtube.com/watch?v=..." --depth deep

# Result: New SPARC mode at .claude/commands/sparc/cot-reasoner.md
# Implementation: 3-4 hours
# Quality: 88% coverage, all gates passed
```

### Workflow 2: Research Paper â†’ New Agent Type
```bash
# Source: "ReAct: Synergizing Reasoning and Acting in Language Models"
/automation:ai-pipeline:extract "path/to/react-paper.pdf"
/automation:ai-pipeline:prd "react-agent"
# ... approve PRD ...
/automation:ai-pipeline:decompose "react-agent-prd"
/automation:ai-pipeline:implement "react-agent-epic"

# Result: New agent type with reasoning/action loop
# Implementation: 4-5 hours
# Quality: 91% coverage, architectural validation passed
```

### Workflow 3: Blog Post â†’ Framework Extension
```bash
# Source: "Building Self-Healing Distributed Systems"
/automation:ai-pipeline "https://blog.example.com/self-healing" --depth standard

# Result: Self-healing extension for Claude Flow
# Implementation: 2-3 hours
# Quality: 85% coverage, integration tests green
```

## ğŸ”§ Troubleshooting

### Common Issues

**Issue**: "Cannot extract concepts from source"
```bash
# Solution 1: Try different depth
/automation:ai-pipeline "url" --depth deep

# Solution 2: Use manual extraction
/automation:ai-pipeline:extract "url"
# Then edit concept manually before proceeding
```

**Issue**: "PRD doesn't capture the concept correctly"
```bash
# Solution: Edit PRD directly
Edit .claude/prds/[concept].md

# Then continue from Phase 3
/automation:ai-pipeline:decompose "[concept]-prd"
```

**Issue**: "Implementation agents stuck or failing"
```bash
# Check status
/monitoring:agents
/pm:epic-status

# View detailed logs
/swarm:swarm-monitor

# If needed, restart specific stream
/pm:issue-start <issue-number>  # Respawns agents for that stream
```

**Issue**: "Quality gates failing"
```bash
# Check which gate failed
/github:pr-manager view <pr-number>

# Common fixes:
# - Coverage too low: Spawn tester agent for more tests
# - Performance regression: Spawn optimizer agent
# - Security issue: Spawn security-engineer agent

# Re-run validation after fixes
/automation:ai-pipeline:validate <pr-number>
```

## ğŸ“Š Performance Tracking

### Measure Your Improvements
```bash
# Before pipeline (baseline)
Time concept â†’ code: [Record manually]
Test coverage: [Check coverage report]
Bug rate: [Track issues found]

# After pipeline (automated)
Time concept â†’ code: ~5.75 hours
Test coverage: 87% average
Bug rate: 0.3 bugs/100 LOC

# Calculate speedup
Speedup = Baseline / Pipeline time
Example: 25 hours / 5.75 hours = 4.3x faster
```

### Track Success Metrics
```yaml
metrics_to_track:
  time_to_implementation:
    - Concept extraction time
    - PRD generation time
    - Implementation time
    - Review time
    - Total time

  quality_metrics:
    - Test coverage percentage
    - Bugs found in review
    - Security vulnerabilities detected
    - Performance regressions

  automation_efficiency:
    - Human approval time
    - Automated vs manual ratio
    - Pipeline success rate
    - Agent utilization
```

## ğŸ¯ Next Steps

### Week 1-2: Validate Foundation
1. âœ… Run pipeline on 3 simple YouTube videos
2. âœ… Measure baseline: time, coverage, quality
3. âœ… Document any issues or improvements needed
4. âœ… Refine approval process based on learnings

### Week 3-4: Scale Complexity
1. âœ… Run pipeline on blog posts with code
2. âœ… Try research paper abstracts
3. âœ… Test with 5-8 parallel agents
4. âœ… Validate quality gates are effective

### Week 5-8: Production Ready
1. âœ… Full research papers â†’ complete systems
2. âœ… Enable Flow-Nexus distributed training for ML concepts
3. âœ… Add continuous learning from outcomes
4. âœ… Measure ROI: time saved, quality improved

## ğŸ”— Related Documentation

- **Full Pipeline Spec**: `.claude/commands/automation/ai-pipeline.md`
- **Architecture Design**: `docs/architecture/autonomous-pipeline-architecture.md`
- **Command Routing**: `.claude/rules/command-routing.md`
- **Agent Capabilities**: `docs/analysis/framework-architecture.md`

## ğŸ’¬ Getting Help

If you encounter issues:

1. **Check documentation**: Read the full pipeline spec
2. **Review logs**: Use `/monitoring:status` and `/swarm:swarm-monitor`
3. **Inspect memory**: Check what was stored at each phase
4. **Ask for help**: Include error messages and phase where stuck

## ğŸ‰ Success Criteria

You've successfully adopted the pipeline when:

âœ… You can go from YouTube video â†’ working code in <6 hours
âœ… Quality gates pass >90% of the time
âœ… Test coverage averages >85%
âœ… Human approval time <30 minutes total
âœ… You trust the pipeline to handle complex concepts

**Ready to transform how you implement AI/ML research?** Start with a simple YouTube video today! ğŸš€
